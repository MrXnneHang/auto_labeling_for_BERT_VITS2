## 这是一个针对Bert-VITS2的一个音频预处理项目.

该项目大部分建立于阿里的Funasr上.<br>
---
### 严禁将此项目用于一切违反《中华人民共和国宪法》，《中华人民共和国刑法》，《中华人民共和国治安管理处罚法》和《中华人民共和国民法典》之用途。

---

说人话是：请尊重别人的隐私，如果有需要公开使用别人的声音作为数据集，请征求别人的同意，至少，是让对方知情。



### 思路：<br>

利用带FunASR上面的带time_stamp的语音识别，获取每句话中第一个字的start_time和最后一个字的end_time.<br>
利用这个start-end把无语音的背景部分排除掉。并且获取每句话的开始和结束<br>

### 项目另一半本身是另一个up做的：@领航员未鸟
项目建立在她的切片语音识别上面。<br>
虽然我写得比她多 <br>
### 注意:

处理完的所有生成文件会放在./tmp下.<br>
待处理的文件需要放在./raw_audio/下<br>

## 如何安装环境:

#### 本身就是funasr的环境适配见：https://alibaba-damo-academy.github.io/FunASR/en/installation/installation.html
## 项目的说明:
适用场景一：给游戏主播录屏(单人音频，大量空白)制作BERT-VITS制作数据集<br>
适用场景二:多人音频，如动漫人物配音提取（兴奋）。以及电台，连麦，视频电话。<br>
适用场景三: 给已经切片好的galgame character wavs 制作BERT-VITS数据集<br>

## 用法:

场景一：<br>
Step 1. Run_clip.py 会按照Funasr的Time_stamp逐句squeeze掉没有说话的音频,而且也会删除音频过短的部分，默认2500ms,可以自行调整 <br>
step 2. 用MDX算法给process后变短的wav降噪,请参考b站领航员未鸟的一键包。|你也可以选择降噪后再进行这个处理，但是通常先删除无音频部分可以让降噪处理时间缩减到原本的四分之一。<br>
Step 3. Run_cut.py  ./raw_audio/降噪后的wavs -> ./tmp/cut/*.wav，根据字幕切割后的音频,每个音频是一个完整长句.<br>
Step 4. Run 10.带标点符号的标注 + 12.清理标注 .用未鸟的auto_labeling来给短音频写esd.list<br>
## 场景一已经基本成熟可以进入测试阶段。
---

场景二:<br>

最常见的场景，多人音频，视频通话，电台，连麦，动漫对话（这个需要日文版本的模型，到时候可以先用国漫来测试）<br>
会根据Pytannote的speaker id 来进行筛选。<br>

经过测试，Pyannote的speaker检测是比较糟糕的，同性别之间的对话经常误判。<br>

改用Funasr自带的speaker标注.效果好很多。但仍在搭建.....<br>

注意场景二完善度依然不够高，如果要用，请把所有音频合成成长音频。然后再进行clip，需要提前了解的是我们使用的是Funasr自带的说话人分割，他的准确度已经很高了，但是依然不足以直接作为数据集来使用。但是却可以用来作为声纹排除一些杂音。<br>
适用场景略显狭窄，因为更加适合于像是访谈的那种场景，但日常中人们对话尤其是在连麦的时候通常声音是重叠的，在声音重叠的时候这个模型表现不佳，识别不到。<br>

考虑只是把它作为一种声纹识别的数据集清洗方式中....<br>


构建ing....
---

场景三:<br>

暂时需要使用Whisper WebUI来获取txt.

Step 1. 初步清洗，删除过短的音频<br>
Step 2. Whisper WebUI来获取*.txt,不需要Time_stamp<br>
step 3. 合成txts成esd.list,txt中可能有几行，这里会把它们合并到同一行，然后在进行写入.<br>
Step 4. 检查一下Whisper 错误生成的文本（乱码，重复单字).这里也可以选择清洗掉过短文本.<br>
Step 5. 已经可以用了<br>

后续会使用Funasr的日文模型。

你可以在b站上联系我：https://space.bilibili.com/556737824?spm_id_from=333.788.0.0
